# Devlog #5

- I made my paper mock up of my text and what they will transition to: next I will plug the text into the physical-book animation to see which effect works best (I feel that it will be splatter or bullet mostly). 
- Not so much project related, but I am going to be working through some Programming Historian tutorials on Wget and web scraping: seems like it would be useful and may as well log my process here. 

## Automated Downloading with Wget tutorial:
- Handily I think I already have Homebrew installed from some previous work when I was trying to get Python to work I believe. Or something along those lines. 
- It was with Pandoc because I can see that it is causing some problems with brew "links". I have proceeded to attempt to install wget we shall see. 
Then this happened: You can try again using:
  brew link wget
==> Summary
üç∫  /usr/local/Cellar/wget/1.19.4_1: 50 files, 3.7MB
Ayanas-MacBook-Pro:~ ayanamorgenstern$ brew link wget
Linking /usr/local/Cellar/wget/1.19.4_1... 
Error: Could not symlink share/man/man1/wget.1
/usr/local/share/man/man1 is not writable.
- Ok! I finally scrolled through the terminal and saw that i had to change the ownership of the file or something and then I was able to link wget. 
- I was able to download the index for active history and am now going to work on downloading more!

Wget operates on the following general basis:
wget [options] [URL]
So let‚Äôs learn a few commands now:

-r
Recursive retrieval is the most important part of wget. What this means is that the program begins following links from the website and downloading them too. So for example, the http://activehistory.ca/papers/ has a link to http://activehistory.ca/papers/historypaper-9/, so it will download that too if we use recursive retrieval. However, it will also follow any other links: if there was a link to http://uwo.ca somewhere on that page, it would follow that and download it as well. By default, -r sends wget to a depth of five sites after the first one. 

--no-parent 
(The double-dash indicates the full-text of a command. All commands also have a short version, this could be initiated using -np).
- Finally, if you do want to go outside of a hierarchy, it is best to be specific about how far you want to go. The default is to follow each link and carry on to a limit of five pages away from the first page you provide. However, perhaps you just want to follow one link and stop there? In that case, you could input -l 2, which takes us to a depth of two web-pages. Note this is a lower-case ‚ÄòL‚Äô, not a number 1.

-l 2
If these commands help direct wget, we also need to add a few more to be nice to servers and to stop any automated countermeasures from thinking the server is under attack! To that end, we have two additional essential commands:

-w 10
It is not polite to ask for too much at once from a web server. There are other people waiting for information, too, and it is thus important to share the load. The command -w 10, then, adds a ten second wait in between server requests. You can shorten this, as ten seconds is quite long. In my own searches, I often use a 2 second wait.
- Another critical comment is to limit the bandwidth you will be using in the download:

--limit-rate=20k

- I think the idea of *cyber etiquette* is super interesting in that there are more "polite" and ethical ways to go about retrieving data and using the web: i had never really considered it. 

wget -r --no-parent -w 2 --limit-rate=20k http://activehistory.ca/papers/ : so this command tells wget to get files from active history papers, using recursive retrieval, with no parent (staying in the hierarchy) with a 2 second wait, and a limit of 20k/s download speed. Neato!
I ran the command and it looks like it's working!
- If you want to mirror an entire website, there is a built-in command to wget.

-m
This command means ‚Äòmirror,‚Äô and is especially useful for backing up an entire website. It introduces the following set of commands: time-stamping, which looks at the date of the site and doesn‚Äôt replace it if you already have that version on your system (useful for repeated downloads), as well as infinite recursion (it will go as many layers into the site as necessary). The command for mirroring ActiveHistory.ca would be:

wget -m -w 2 --limit-rate=20k http://activehistory.ca

## Applied Archival Downloading Tutorial

Let‚Äôs get started. The first step involves building a script to generate sequential URLs using Python‚Äôs ForLoop function. First, you‚Äôll need to identify the beginning URL in the series of documents that you want to download. Because of its smaller size we‚Äôre going to use the online war diary for No. 14 Canadian General Hospital as our example. The entire war diary is 80 pages long. The URL for page 1 is http://data2.archives.ca/e/e061/e001518029.jpg and the URL for page 80 is ‚Äòhttp://data2.archives.ca/e/e061/e001518109.jpg. Note that they are in sequential order. We want to download the .jpeg images for all of the pages in the diary. To do this, we need to design a script to generate all of the URLs for the pages in between (and including) the first and last page of the diary.

Open your preferred text editor (such as Komodo Edit) and enter the code below. Where it says ‚Äòinteger 1‚Ä≤ type in ‚Äò8029‚Ä≤, where it says ‚Äòinteger 2‚Ä≤, type ‚Äò8110‚Äô. The ForLoop will generate a list of numbers between ‚Äò8029‚Äô and ‚Äò8110‚Äô, but it will not print the last number in the range (i.e. 8110). To download all 80 pages in the diary you must add one to the top-value of the range because it is at this integer where the ForLoop is told to stop
urls = '';
f=open('urls.txt','w')
for x in range('integer1', 'integer2'):
    urls = 'http://data2.collectionscanada.ca/e/e061/e00151%d.jpg\n' % (x)
    f.write(urls)
f.close

It was going well until I came to the spot where I'm supposed to save it as a .py and run the script? How does one actually do this or am I supposed to just move on to the next step? I'm going to take a break sweet lord. 
- I have decided to use Komodo edit in an attempt to mitigate this problem? This way I am following the tutorial as closely as possible. 
- Still not working, there are some issues when I try to get it to pull the script/file of urls. 

